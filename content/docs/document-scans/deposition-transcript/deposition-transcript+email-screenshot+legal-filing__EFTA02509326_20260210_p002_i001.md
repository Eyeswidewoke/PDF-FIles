---
source_image: "deposition-transcript+email-screenshot+legal-filing__EFTA02509326_20260210_p002_i001.png"
source_pdf: "EFTA02509326.pdf"
method: pdf_text
words: 664
confidence: 1.00
extracted: 2026-02-13T16:38:50.113424
---

» Last week I got to know Steve Hyman, Daniel Kahneman and Bob 
> =orvitz. Telefonica invited all of us to a two day workshop with Pablo =odriguez, Ken Morse and a few others, where 
we were meant to advise =hem on how to use Al for health applications. I told them that I think =he goal of therapeutic 
invention is not to increase happiness, but =ntegrity. Happiness is merely an indicator, not the benchmark. Current =pps 
tend to subvert the motivation of people, but I don't think that =his is necessary or the best strategy. Humans are meant 
to be =rogrammable, not subverted. They perceive their programming as "higher =urpose". If we can come from the top, 
supporting purpose, instead of =rom the bottom, subverting attention, we might be more successful. =Downside might 
be that we create cults.) Of the bunch, Hyman managed to be the most interesting (Kahneman was =ery charismatic but 
mostly tried to see if he could identify an =pplication for his system one/system two theory). Gary Marcus was =here, 
too, but annoyed everyone by being too insecure to deal with his =ncompetence. 
> > 
» Did I tell you that I discovered that Deep Learning might be best =nderstood as Second order AI? 
> > 
» First order Al was the classical Al that was started by Marvin =insky in the 1950ies, and it worked by figuring out how 
we (or an =bstract system) can perform a task that requires intelligence, and then =mplementing that algorithm directly. 
It yielded most of the progress we =aw until recently: chess programs, data bases, language parsers etc. 
> Second order Al does not implement the functionality directly, but =e write the algorithms that figure out the 
functionality by themselves. =econd order Al is automated function approximation. Learning has =xisted for a long time 
in Al of course, but Deep Learning means =ompositional function approximation. 
> Our current approximator paradigm is mostly the neural network, i.e. =hained normalized weighted sums of real 
values that we adapt by =hanging the weights with stochastic gradient descent, using the chain =ule. This works well for 
linear algebra and the fat end of compact =olynomials, but it does not work well for conditional loops, recursion =nd 
many other constructs that we might want to learn. Ultimately, we =ant to learn any kind of algorithm that runs 
efficiently on the =vailable hardware. 
» Neural network learning is very slow. The different learning =lgorithms are quite similar in the amount of structure 
they can squeeze =ut of the same training data, but they need far more passes over the =ata than our nervous system. 
> The solution might be meta learning: we write algorithms that learn =ow to create learning algorithms. Evolution is 
meta learning. Meta =earning is going to be third order Al and perhaps trigger a similar =ave as deep learning. 
> > 
» I intend to visit NYC for a workshop at NYU on the weekend of the =6th. 
> > 
> We just moved into a new apartment; the previous one had only two =edrooms and this one has three, so I can have 
a study. It seems that we =re as lucky with the new landlords as with the previous ones. 
> > 
» Bests, and thank you for everything! 
> > 
> Joscha 
> > 
> > 
> > 
> > > On Mar 8, 2018, at 16:37, jeffrey E. <jeevacation@gmail.com> =rote: 
>>  >
> > > progress? 
>>  >
> > > 
> > > 
please note 
> >  The information contained in this communication is confidential, 
> >  may be attorney-client privileged, may constitute inside 
> » information, and is intended only for the use of the addressee. It 
> >  is the property of JEE Unauthorized use, disclosure or copying of 
> >  this communication or any part thereof is strictly prohibited and 
> > > may be unlawful. If you have received this communication in error, 
2 
EFTA_R1_01640592 
EFTA02509327
