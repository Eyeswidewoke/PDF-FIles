---
source_image: "deposition-transcript+legal-filing+court-document__EFTA02508316_20260210_p001_i001.png"
source_pdf: "EFTA02508316.pdf"
method: pdf_text
words: 560
confidence: 1.00
extracted: 2026-02-13T16:50:50.661348
---

From: 
jeffrey E. <jeevacation@gmail.com> 
Sent: 
Friday, March 9, 2018 10:40 AM 
To: 
Joscha Bach 
Subject: 
Re: 
I would think of it more of a space / field effects = Not recursive algorithm s 
> wrote: 
Last week I got to know Steve Hyman, Daniel Kahneman and Bo= Horvitz. Telefonica invited all of us to a two 
day workshop with Pablo Ro=riguez, Ken Morse and a few others, where we were meant to advise them on =ow to use 
Al for health applications. I told them that I think the goal of=therapeutic invention is not to increase happiness, but 
integrity. Happine=s is merely an indicator, not the benchmark. Current apps tend to subvert =he motivation of people, 
but I don't think that this is necessary or t=e best strategy. Humans are meant to be programmable, not subverted. They 
=erceive their programming as "higher purpose". If we can come fr=m the top, supporting purpose, instead of from the 
bottom, subverting atte=tion, we might be more successful. (Downside might be that we create cults=) 
Of the bunch, Hyman managed to be the most interesting (Kahneman was very c=arismatic but mostly tried to 
see if he could identify an application for =is system one/system two theory). Gary Marcus was there, too, but annoyed 
=veryone by being too insecure to deal with his incompetence. 
Did I tell you that I discovered that Deep Learning might be best understoo= as Second order Al? 
First order Al was the classical Al that was started by Marvin Minsky in th= 1950ies, and it worked by figuring out 
how we (or an abstract system) can=perform a task that requires intelligence, and then implementing that algo=ithm 
directly. It yielded most of the progress we saw until recently: ches= programs, data bases, language parsers etc. 
Second order Al does not implement the functionality directly, but we write=the algorithms that figure out the 
functionality by themselves. Second ord=r Al is automated function approximation. Learning has existed for a long =ime 
in Al of course, but Deep Learning means compositional function approx=mation. 
Our current approximator paradigm is mostly the neural network, i.e. chaine= normalized weighted sums of real 
values that we adapt by changing the wei=hts with stochastic gradient descent, using the chain rule. This works wel= for 
linear algebra and the fat end of compact polynomials, but it does no= work well for conditional loops, recursion and 
many other constructs that=we might want to learn. Ultimately, we want to learn any kind of algorithm=that runs 
efficiently on the available hardware. 
Neural network learning is very slow. The different learning algorithms are=quite similar in the amount of 
structure they can squeeze out of the same =raining data, but they need far more passes over the data than our 
nervous=system. 
The solution might be meta learning: we write algorithms that learn how to =reate learning algorithms. 
Evolution is meta learning. Meta learning is go=ng to be third order Al and perhaps trigger a similar wave as deep 
learnin=. 
I intend to visit NYC for a workshop at NYU on the weekend of the 16th. 
We just moved into a new apartment; the previous one had only two bedrooms =nd this one has three, so I can 
have a study. It seems that we are as luck= with the new landlords as with the previous ones. 
Bests, and thank you for everything! 
I 
EFTA_R1_01639307 
EFTA02508316
